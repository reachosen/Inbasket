# -*- coding: utf-8 -*-
"""InbasketDebertaDifferences.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kw4mZg3D8s_1affD8p7Jipye1HT-IR3K
"""

pip install transformers

pip install sentencepiece

!huggingface-cli login

"""This is huggingface trained code"""

from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained("reachosen/autotrain-inbasket3.5-97183146829", use_auth_token=True)

tokenizer = AutoTokenizer.from_pretrained("reachosen/autotrain-inbasket3.5-97183146829", use_auth_token=True)

import torch

# Calculate the total correct matches
total_matches = (data['Category'] == data['Predicted_Category']).sum()
print(f"Total Correct Matches: {total_matches}")

import torch
import pandas as pd
import numpy as np

# Assume you've initialized 'tokenizer' and 'model' here

# Step 1: Load the CSV file
file_path = "/content/sample_data/Inbasketv3.45SP.csv"
data = pd.read_csv(file_path)

# Step 1.5: Handle missing or invalid data in 'Test Case' column
data['Test Case'].fillna('Unknown', inplace=True)  # Fill NaN values
data['Test Case'] = data['Test Case'].apply(str)  # Ensure all data is string type

# Step 2: Tokenize and Predict labels for "Test Case"
try:
    inputs = tokenizer(data['Test Case'].tolist(), truncation=True, padding=True, return_tensors='pt')
    with torch.no_grad():
        outputs = model(**inputs)
        predictions = torch.argmax(outputs.logits, dim=1).cpu().numpy()
except Exception as e:
    print(f"An error occurred: {e}")
    predictions = np.full(len(data), -1)  # Fill with -1 or another invalid label ID

# Assume you've initialized 'label_mapping' here

# Step 1: Define label mapping (using the provided id2label mapping)
label_mapping = {
    "0": "Abdominal N",
    "1": "Abdominal U",
    "2": "Admin/Supplies - N",
    "3": "Admin/Supplies - U",
    "4": "Behavioral Health - N",
    "5": "Behavioral Health - U",
    "6": "BrkB N",
    "7": "BrkB U",
    "8": "Digestive N",
    "9": "Digestive U",
    "10": "Face/Tongue/Lip Swell N",
    "11": "Face/Tongue/Lip Swell U",
    "12": "Hd N",
    "13": "Hd U",
    "14": "Lg Ank Swling N",
    "15": "Lg Ank Swling U",
    "16": "Limb N",
    "17": "Limb U",
    "18": "Mov N",
    "19": "Mov U",
    "20": "Musculoskeletal N",
    "21": "Musculoskeletal U",
    "22": "Neuro - N",
    "23": "Neuro - U",
    "24": "Respiratory N",
    "25": "Respiratory U",
    "26": "Seizure N",
    "27": "Seizure U",
    "28": "Sk N",
    "29": "Sk U",
    "30": "Stand up/Bear Weight N",
    "31": "Stand up/Bear Weight U",
    "32": "Throat N",
    "33": "Throat U",
    "34": "Vision N",
    "35": "Vision U"
}

# Step 2: Map predictions to label names
data['Predicted_Category'] = [label_mapping.get(str(i), 'Unknown') for i in predictions]

# Step 3: Identify misclassifications
misclassifications = data[data['Category'] != data['Predicted_Category']]

# Step 4: Analyze failed categories
failed_category_distribution = misclassifications['Category'].value_counts()
print(failed_category_distribution)

misclassifications

data
